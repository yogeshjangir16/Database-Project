# -*- coding: utf-8 -*-
"""project_operation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SJmIUd2VJOUNgzlobao0rP3Cq-3sXdHM

*   **Normalization**
*   **Data Recovery**
*   **Query Optimization**
*   **Cascading**
"""

import pandas as pd
import time
import os
import shutil

admin = pd.read_csv('/content/admin.csv')
author = pd.read_csv('/content/author.csv')
book = pd.read_csv('/content/books.csv')
category = pd.read_csv('/content/category.csv')
issue_book = pd.read_csv('/content/issue_books.csv')
user = pd.read_csv('/content/users.csv')

print(admin)

print(author)

print(book)

print(category)

print(issue_book)

print(user)

"""# Relation"""

admin.columns = admin.columns.str.strip()
author.columns = author.columns.str.strip()
book.columns = book.columns.str.strip()
category.columns = category.columns.str.strip()
user.columns = user.columns.str.strip()
issue_book.columns = issue_book.columns.str.strip()

book_author_merge = book.merge(author, on='author_id', how='inner')
book_cat_merge = book.merge(category, on='category_id', how='inner')
book_issue_merge = book.merge(issue_book, on='book_no', how='inner')
print(book_author_merge)

print(book_cat_merge)

"""## Query Optimization"""

# Find authors and book they have category is "CSE"
merged_data1 = book_author_merge.merge(category,on='category_id', how='inner')
print(merged_data1)

cse_books_authors = merged_data1[merged_data1['category_name'] == 'CSE']
print(cse_books_authors)

# Commented out IPython magic to ensure Python compatibility.
# %timeit -n 5 -r 2 merged_data = book_author_merge.merge(book_cat_merge, on='category_id', how='inner'); cse_books_authors = merged_data[merged_data['category_name'] == 'CSE']

cse_category_number = category.loc[category['category_name'] == 'CSE', 'category_id'].iloc[0]
print("Category Number for 'CSE':", cse_category_number)

cse_books = book[book['category_id'] == cse_category_number]
cse_authors = author[author['author_id'].isin(cse_books['author_id'])]
print(cse_authors)

# Commented out IPython magic to ensure Python compatibility.
def calculate_time():
    cse_category_number = category.loc[category['category_name'] == 'CSE', 'category_id'].iloc[0]
    cse_books = book[book['category_id'] == cse_category_number]
    cse_authors = author[author['author_id'].isin(cse_books['author_id'])]
# %timeit -r 2 -n 1 calculate_time()

"""we can see that the time required for by using join operation is 5.1ms while using subquery it's 12.5ms hence we have correctly optimized .

## **Recovery**
"""

admin_path = '/content/admin.csv'
author_path = '/content/author.csv'
book_path = '/content/books.csv'
category_path = '/content/category.csv'
issue_book_path = '/content/issue_books.csv'
user_path = '/content/users.csv'

backup_directory = '/content/backup'
os.makedirs(backup_directory, exist_ok=True)

backup_admin_path = os.path.join(backup_directory, 'admin_backup.csv')
backup_author_path = os.path.join(backup_directory, 'author_backup.csv')
backup_book_path = os.path.join(backup_directory, 'books_backup.csv')
backup_category_path = os.path.join(backup_directory, 'category_backup.csv')
backup_issue_book_path = os.path.join(backup_directory, 'issue_books_backup.csv')
backup_user_path = os.path.join(backup_directory, 'users_backup.csv')

shutil.copy(admin_path, backup_admin_path)
shutil.copy(author_path, backup_author_path)
shutil.copy(book_path, backup_book_path)
shutil.copy(category_path, backup_category_path)
shutil.copy(issue_book_path, backup_issue_book_path)
shutil.copy(user_path, backup_user_path)
print(f"Backup completed.Location: {backup_directory}")

print(admin)

admin_backup = pd.read_csv('/content/backup/admin_backup.csv')
print(admin_backup)

"""As we can see that Recovery Successfully happed.

# Materlization View
"""

view = book_cat_merge[book_cat_merge['category_name'] == 'CSE']
print(view.head())

view = book_cat_merge[book_cat_merge['category_name'] == 'Bio']
print(view.head())

view = book_author_merge[book_author_merge['author_name'] == 'Author2']
print(view.head())

"""# **Data Optimization**"""

optimal_data = merged_data.copy()
print(optimal_data.info())

"""Memory Usage: 3.1KB"""

optimal_data.rename(columns={
    'book_name_x': 'book_name',
    'author_id_x': 'author_id',
    'category_id': 'category_id',
    'book_no_x': 'book_no',
    'book_price_x': 'book_price',
    'author_name': 'author_name',
    'mobile': 'mobile',
    'address': 'address',
    'book_name_y': 'related_book_name',
    'author_id_y': 'related_author_id',
    'book_no_y': 'related_book_no',
    'book_price_y': 'related_book_price',
    'category_name': 'related_category_name'
}, inplace=True)
optimal_data = optimal_data.loc[:, ~optimal_data.columns.duplicated()]
categorical_columns = ['book_name', 'author_name', 'address', 'related_book_name', 'related_category_name']
for column in categorical_columns:
    optimal_data[column] = optimal_data[column].astype('category')
int_columns = ['author_id', 'category_id', 'book_no', 'book_price', 'mobile', 'related_author_id', 'related_book_no', 'related_book_price']
optimal_data[int_columns] = optimal_data[int_columns].apply(pd.to_numeric, downcast='integer')
print(optimal_data.info())

"""# **Sharding**"""

shard1_range = merged_data1[merged_data1['category_id'] <= 10005]
shard2_range = merged_data1[(merged_data1['category_id'] > 10005) & (merged_data1['category_id'] <= 10007)]

num_shards = 3
merged_data1['hash_shard'] = merged_data1['category_id'].apply(lambda x: hash(x) % num_shards)
shard1_hash = merged_data1[merged_data1['hash_shard'] == 0]
shard2_hash = merged_data1[merged_data1['hash_shard'] == 1]
shard3_hash = merged_data1[merged_data1['hash_shard'] == 2]

print("\nHash Sharding Example - Shard 1:")
print(shard1_hash.head())

print("\nHash Sharding Example - Shard 2:")
print(shard2_hash.head())

print("\nHash Sharding Example - Shard 3:")
print(shard3_hash.head())